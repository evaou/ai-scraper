# ü§ñ AI-Powered Web Scraper

[![Docker](https://img.shields.io/badge/Docker_Compose-v2.39+-blue.svg)](https://docs.docker.com/compose/)
[![Python](https://img.shields.io/badge/Python-3.11+-green.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.112+-orange.svg)](https://fastapi.tiangolo.com/)
[![Playwright](https://img.shields.io/badge/Playwright-1.48+-purple.svg)](https://playwright.dev/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15+-blue.svg)](https://www.postgresql.org/)
[![Redis](https://img.shields.io/badge/Redis-7+-red.svg)](https://redis.io/)

> **Status**: ‚úÖ **PRODUCTION READY** - Deployed and running in production with automated CI/CD

A sophisticated, production-ready web scraping system that leverages Large Language Models (LLMs) to intelligently extract structured data from web pages. Features distributed architecture with automatic scaling, comprehensive monitoring, and zero-downtime deployment pipeline.

## üìä **Project Status**

‚úÖ **Codebase**: 49 Python files, 74 unit tests passing  
‚úÖ **Architecture**: FastAPI + Pydantic V2, fully containerized  
‚úÖ **CI/CD Pipeline**: Automated GitHub Actions with health checks  
‚úÖ **Production Features**: 2 API replicas + 3 workers + load balancer  
‚úÖ **Enterprise Ready**: Redis auth, monitoring, auto-rollback

---

## üåü Features

### **üöÄ Core Capabilities**

- **AI-Powered Data Extraction**: Uses OpenAI GPT models to intelligently parse and structure web content
- **Advanced Browser Automation**: Playwright-based scraping with stealth capabilities and JavaScript execution
- **Distributed Architecture**: Redis-backed task queue system with horizontal scaling
- **Smart Anti-Detection**: Rotating user agents, proxy support, and human-like browsing patterns
- **Flexible Output Formats**: Support for JSON, CSV, and custom data structures

### **üèóÔ∏è Production Features**

- **RESTful API**: FastAPI-based web interface with OpenAPI documentation
- **Load Balancing**: Nginx reverse proxy with multiple API replicas
- **Comprehensive Monitoring**: Prometheus metrics and health checks
- **Automated Deployment**: GitHub Actions CI/CD with zero-downtime deployment
- **Database Management**: PostgreSQL with Alembic migrations
- **Caching & Queuing**: Redis for session management and task distribution
- **Container Orchestration**: Docker Compose with service discovery
- **Security**: JWT authentication, API rate limiting, and secure secrets management

---

## üöÄ Quick Start

### **Prerequisites**

- **Docker & Docker Compose** (v2.20+)
- **Python 3.11+** (for local development)
- **Git** for version control

### **Local Development Setup**

```bash
# 1. Clone the repository
git clone https://github.com/evaou/ai-scraper.git
cd ai-scraper

# 2. Create environment file
cp .env.example .env
# Edit .env with your configuration

# 3. Start the development stack
docker compose up -d

# 4. Verify services are running
docker compose ps
# Expected: api, worker, db, redis all "Up"

# 5. Test the API
curl http://localhost:8000/health
# Expected: {"status": "healthy"}
```

### **Basic Usage**

#### **Example API Request**

```bash
curl -X POST "http://localhost:8000/api/v1/scrape" \
-H "Content-Type: application/json" \
-d '{
  "url": "https://example.com",
  "selector": "h1",
  "options": {
    "javascript_enabled": true,
    "wait_for": "networkidle",
    "timeout": 30
  }
}'
```

**Response:**
```json
{
  "job_id": "uuid-string",
  "status": "pending",
  "url": "https://example.com/",
  "created_at": "2025-10-04T23:35:53.644155+00:00",
  "estimated_completion": "2025-10-04T23:36:23.670375",
  "priority": 0
}
```

## üì± Client Applications

### **USD Rate Scraper - Production Ready ‚úÖ**

```bash
# Get current USD rate (quiet mode)
cd client/
./get_usd_rate.sh
# Output: 30.435

# Run with detailed output
python3 usd_rate_scraper.py --url "https://httpbin.org/html" --verbose
```

---

## üèóÔ∏è Architecture

### **Project Structure**

```
ai-scraper/
‚îú‚îÄ‚îÄ üê≥ docker-compose.yml        # Development environment
‚îú‚îÄ‚îÄ üê≥ docker-compose.prod.yml   # Production deployment
‚îú‚îÄ‚îÄ üìÑ pyproject.toml           # Python dependencies
‚îú‚îÄ‚îÄ üìÑ alembic.ini              # Database migrations
‚îú‚îÄ‚îÄ üìÅ app/                     # Main application
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py              # FastAPI application
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/                 # API routes
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                # Core configuration
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ crud/                # Database operations
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/              # SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ schemas/             # Pydantic schemas
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ services/            # Business logic
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ worker/              # Background workers
‚îú‚îÄ‚îÄ üìÅ tests/                   # Test suite
‚îú‚îÄ‚îÄ üìÅ client/                  # Client applications
‚îú‚îÄ‚îÄ üìÅ scripts/                 # Deployment scripts
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml       # Development environment
‚îî‚îÄ‚îÄ üìÑ .github/workflows/       # CI/CD automation
```

### **Technology Stack**

**Backend & API**
- FastAPI 0.112+ (Async Python web framework)
- Pydantic v2 (Data validation & serialization)
- SQLAlchemy 2.0+ (Database ORM)
- Alembic (Database migrations)

**Scraping & AI**
- Playwright 1.48+ (Browser automation)
- OpenAI GPT API (Content extraction)
- BeautifulSoup4 (HTML parsing)

**Infrastructure**
- PostgreSQL 15+ (Primary database)
- Redis 7+ (Caching & task queue)
- Nginx (Load balancer & reverse proxy)
- Docker & Docker Compose (Containerization)

**Monitoring & DevOps**
- Prometheus (Metrics collection)
- GitHub Actions (CI/CD pipeline)
- pytest (Testing framework)
- Ruff (Code linting & formatting)

---

## üöÄ Production Deployment

### **üìã Automated CI/CD Pipeline**

**‚úÖ Production Ready Features:**
- [x] Automated CI/CD with GitHub Actions
- [x] Zero-downtime deployment with health checks  
- [x] Automatic rollback on deployment failure
- [x] Multi-replica load balancing with Nginx
- [x] Comprehensive monitoring with Prometheus
- [x] Secure secrets management
- [x] Redis authentication and PostgreSQL optimization

### **üñ•Ô∏è Step 1: Create & Configure Server**

**1.1 Create Linode Instance**
1. Go to [Linode Cloud Manager](https://cloud.linode.com)
2. Create new Linode: Ubuntu 22.04 LTS, Nanode 1GB ($5/month) minimum
3. Set root password and note your server IP

**1.2 Automated Server Setup**
```bash
# SSH to your server
ssh root@YOUR_SERVER_IP

# Download and run setup script
wget https://raw.githubusercontent.com/evaou/ai-scraper/main/linode-setup.sh
chmod +x linode-setup.sh
./linode-setup.sh

# Validate setup
wget https://raw.githubusercontent.com/evaou/ai-scraper/main/validate-deployment.sh
chmod +x validate-deployment.sh
./validate-deployment.sh
```

### **üîê Step 2: Configure GitHub Secrets**

**Go to:** GitHub Repository ‚Üí **Settings** ‚Üí **Secrets and variables** ‚Üí **Actions**

**Required Secrets (8 total):**

| Secret Name         | Value                 | Example              |
|--------------------|-----------------------|----------------------|
| `LINODE_HOST`      | Your Linode server IP | `192.168.1.100`      |
| `LINODE_USER`      | SSH username          | `root`               |
| `SSH_PRIVATE_KEY`  | Private key content   | Generate with script |
| `POSTGRES_DB`      | Database name         | `scraper_prod`       |
| `POSTGRES_USER`    | Database username     | `scraper_user`       |
| `POSTGRES_PASSWORD`| Database password     | Generate with script |
| `REDIS_PASSWORD`   | Redis password        | Generate with script |
| `JWT_SECRET_KEY`   | JWT signing secret    | Generate with script |

**Generate Secure Passwords:**
```bash
# Run locally to generate passwords
./generate-secrets.sh
```

### **üéØ Step 3: Deploy**

**Option A: Automatic Deploy (Recommended)**
```bash
git add .
git commit -m "feat: deploy to production"
git push origin main
# ‚úÖ GitHub Actions automatically deploys with health checks!
```

**Option B: Manual Trigger**
1. Go to GitHub ‚Üí Actions ‚Üí "Deploy to Production"
2. Click "Run workflow" ‚Üí "Run workflow"
3. Monitor deployment progress

### **üìä Step 4: Monitor Deployment**

**Health Check Endpoints:**
```bash
# API Health Check
curl http://YOUR_SERVER_IP/health

# Full API Documentation  
http://YOUR_SERVER_IP/docs

# Metrics (Prometheus)
http://YOUR_SERVER_IP/metrics
```

**Container Status:**
```bash
# SSH to server and check
ssh root@YOUR_SERVER_IP
docker compose -f /opt/ai-scraper/docker-compose.prod.yml ps

# Expected output:
# NAME                    STATUS                    PORTS
# ai-scraper-api-1        Up (healthy)             
# ai-scraper-api-2        Up (healthy)             
# ai-scraper-worker-1     Up (healthy)             
# ai-scraper-worker-2     Up (healthy)             
# ai-scraper-worker-3     Up (healthy)             
# scraper_db_prod         Up (healthy)             5432/tcp
# scraper_nginx_prod      Up                       0.0.0.0:80->80/tcp
# scraper_redis_prod      Up (healthy)             6379/tcp
```

---

## ‚ö° Performance & Scaling

### **üìà Current Architecture Performance**

**Load Balancing & Scaling:**
- **API Replicas**: 2 instances behind Nginx load balancer
- **Worker Processes**: 3 dedicated scraping workers  
- **Database**: PostgreSQL with connection pooling
- **Cache**: Redis with authentication for session management
- **Auto-scaling**: Can dynamically scale workers based on queue length

**Performance Benchmarks:**
- **Health Check**: < 50ms
- **Job Submission**: < 200ms  
- **Result Retrieval**: < 100ms
- **Concurrent Users**: 100+ supported
- **Basic Scraping**: 1-5 seconds per URL
- **Complex Scraping**: 10-30 seconds per URL
- **Queue Throughput**: 20+ jobs/minute
- **Parallel Processing**: 3 simultaneous scraping tasks

### **üîß Scaling Strategies**

**Horizontal Scaling:**
```bash
# Scale API replicas
docker compose up -d --scale api=5

# Scale worker processes  
docker compose up -d --scale worker=10
```

**Resource Allocation:**
```yaml
# Production resource limits (docker-compose.prod.yml)
api:
  deploy:
    replicas: 2
    resources:
      limits:
        memory: 512M
        cpus: "0.5"
      
worker:  
  deploy:
    replicas: 3
    resources:
      limits:
        memory: 1024M
        cpus: "1.0"
```

**Cost Optimization:**
- **Memory Usage**: ~2GB total for full stack
- **CPU Usage**: ~2 cores for high throughput
- **Linode 2GB**: $12/month (recommended minimum)
- **Linode 4GB**: $24/month (high performance)

---

## üîß API Reference

### **Monitoring Endpoints**

```bash
# Job statistics
curl "http://localhost:8000/api/v1/stats"

# Admin statistics (detailed)
curl "http://localhost:8000/api/v1/admin/stats"

# Queue status
curl "http://localhost:8000/api/v1/admin/queue"
```

---

## üóÑÔ∏è Database Management

### **Migrations**
```bash
# Check current migration status
docker compose exec api alembic current

# Apply migrations
docker compose exec api alembic upgrade head

# Create new migration
docker compose exec api alembic revision --autogenerate -m "Description"
```

### **Database Access**
```bash
# Connect to PostgreSQL
docker compose exec db psql -U scraper_user -d scraper_db

# View tables
\dt

# View job data
SELECT * FROM jobs ORDER BY created_at DESC LIMIT 10;
```

---

## üîß Troubleshooting

### **Common Deployment Issues**

**‚ùå GitHub Actions Deployment Fails**
```bash
# Check deployment logs in GitHub Actions tab
# Common fixes applied in latest version:

# 1. Redis Authentication Issues ‚Üí Fixed with individual env vars
# 2. Port Conflicts ‚Üí Fixed with proper service dependencies  
# 3. Health Check Failures ‚Üí Fixed with extended wait times
# 4. Nginx Upstream Errors ‚Üí Fixed with prometheus dependency
```

**üîÑ Automatic Rollback Triggered**
- The deployment includes **intelligent rollback**
- Triggers on health check failures
- Restores previous working configuration
- Maintains service availability
- Logs rollback reason in GitHub Actions

**üìä Service Health Issues**
```bash
# SSH to server for detailed diagnostics
ssh root@YOUR_SERVER_IP
cd /opt/ai-scraper

# Check service logs
docker compose logs api --tail=50
docker compose logs nginx --tail=20
docker compose logs redis --tail=20

# Restart specific service if needed
docker compose restart api
```

### **Common Development Issues**

**Port already in use:**
```bash
# Find process using port 8000
lsof -i :8000
# Kill the process
kill -9 <PID>
```

**Database connection error:**
```bash
# Reset database
docker compose down -v
docker compose up -d db
# Wait for startup, then start other services
docker compose up -d
```

**Performance Tuning:**
```bash
# Monitor resource usage
docker stats

# Scale workers for high load
docker compose up -d --scale worker=5

# Check queue status
curl "http://localhost:8000/api/v1/admin/queue"
```

---

## üîÑ Maintenance & Updates

### **Updating the Application**
Simply push to `main` branch - GitHub Actions handles everything:
```bash
git add .
git commit -m "feat: new feature"
git push origin main
# ‚úÖ Automatic deployment with health checks and rollback
```

### **Log Management**
```bash
# View logs with timestamps
docker compose logs -f --timestamps api

# Rotate logs (automated via Docker)
docker system prune -f --volumes
```

---

## üß™ Testing

### **Running Tests**
```bash
# Activate virtual environment
source venv/bin/activate

# Install test dependencies
pip install -e ".[test]"

# Run unit tests
pytest tests/unit/ -v
```

### **Testing API with Client Scripts**
```bash
# Test USD Rate Scraper (working)
cd client/
python3 usd_rate_scraper.py --url "https://httpbin.org/html" --verbose

# Test shell wrapper (working)
./demo.sh
```

---

## ‚öôÔ∏è System Requirements

### **Tested Environment**

| Component          | Version              | Status        |
| ------------------ | -------------------- | ------------- |
| **OS**             | macOS 26.0.1 (ARM64) | ‚úÖ Tested     |
| **Python**         | 3.13.3               | ‚úÖ Compatible |
| **Docker**         | 27.3.1               | ‚úÖ Working    |
| **Docker Compose** | v2.39.4              | ‚úÖ Working    |
| **PostgreSQL**     | 15-alpine            | ‚úÖ Healthy    |
| **Redis**          | 7-alpine             | ‚úÖ Healthy    |

### **Performance**
- **API Response Time**: < 100ms for health checks
- **Scraping Jobs**: Complete in 2-5 seconds for simple pages
- **Worker Processing**: 2 concurrent workers by default
- **Database**: Handles 1000+ jobs without issues

---

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Run tests: `pytest tests/unit/ -v`
4. Run linting: `ruff check app/`
5. Submit a pull request

---

## üÜò Support

- **Issues**: Create a GitHub issue for bugs or feature requests
- **Documentation**: See inline code documentation
- **Examples**: Check the `client/` directory for working examples

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**üöÄ Production Ready!** Your AI Scraper deployment includes:
- ‚ö° **Auto-scaling**: 2 API replicas + 3 worker processes  
- üîÑ **Zero-downtime**: Rolling deployments with health checks
- üìä **Monitoring**: Prometheus metrics + comprehensive logging
- üõ°Ô∏è **Security**: Redis authentication + environment isolation
- üîß **Automation**: Full CI/CD pipeline with automatic rollback
- üìà **Performance**: Nginx load balancing + Redis caching

**Need Help?** Check logs in GitHub Actions or SSH to server for detailed diagnostics.

---

**Last Updated**: October 2025 | **Version**: 1.0.0 | **Status**: Production Ready ‚úÖ