services:
  # PostgreSQL Database
  db:
    image: postgres:15-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-scraper}
      POSTGRES_USER: ${POSTGRES_USER:-postgres}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-postgres} -d ${POSTGRES_DB:-scraper}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - scraper_network

  # Redis Cache
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - scraper_network

  # FastAPI Application
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    restart: unless-stopped
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:password@db:5432/scraper
      - REDIS_URL=redis://redis:6379/0
      - DEBUG=false
      - RELOAD=false
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - API_KEY_REQUIRED=false
      - CORS_ORIGINS=["*"]
      - STORAGE_PATH=/tmp/scraper-storage
    volumes:
      - ./app:/app/app:ro
      - storage_data:/tmp/scraper-storage
    ports:
      - "8000:8000"
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - scraper_network

  # Scraper Worker
  worker:
    build:
      context: .
      dockerfile: docker/Dockerfile.worker
    restart: unless-stopped
    environment:
      - DATABASE_URL=postgresql+asyncpg://postgres:password@db:5432/scraper
      - REDIS_URL=redis://redis:6379/0
      - CELERY_BROKER_URL=redis://redis:6379/1
      - CELERY_RESULT_BACKEND=redis://redis:6379/2
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - PLAYWRIGHT_BROWSER=chromium
      - WORKER_CONCURRENCY=2
      - STORAGE_PATH=/tmp/scraper-storage
      # Playwright environment
      - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
    volumes:
      - ./app:/app/app:ro
      - storage_data:/tmp/scraper-storage
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_healthy
      api:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    healthcheck:
      test: ["CMD", "python", "-c", "import redis; r = redis.Redis(host='redis', port=6379, db=0); r.ping()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - scraper_network

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  storage_data:
    driver: local

networks:
  scraper_network:
    driver: bridge